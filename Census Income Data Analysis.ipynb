{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Income Prediction Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The US Census reports a vast dataset of demographic information about people living in the USA. A subset of this data which has been munged to a certain degree can be found [here](https://archive.ics.uci.edu/ml/datasets/Census+Income) with 15 features, including income information.\n",
    "\n",
    "In this analysis, I built two models using random forest classifier (RFC) and losgistic regression (LG). RFC achieved 85.1% and 84.9% accuracy on the training data and the test data seperately, while LG got 77.2% and 77.0% for training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in data and feature conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                int64\n",
       "workclass         object\n",
       "fnlwgt             int64\n",
       "education         object\n",
       "education_num      int64\n",
       "marital_status    object\n",
       "occupation        object\n",
       "relationship      object\n",
       "race              object\n",
       "sex               object\n",
       "capital_gain       int64\n",
       "capital_loss       int64\n",
       "hours_per_week     int64\n",
       "native_country    object\n",
       "income            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colnames = ['age', 'workclass', 'fnlwgt', 'education', \\\n",
    "            'education_num', 'marital_status', 'occupation', \\\n",
    "            'relationship', 'race', 'sex', 'capital_gain', \\\n",
    "            'capital_loss', 'hours_per_week', 'native_country',\\\n",
    "            'income']\n",
    "income_data = pd.read_csv(\"adult.data.csv\", \n",
    "                          names = colnames)\n",
    "income_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing, we need to check if there's any duplicated information in the data and then remove them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "income_data = income_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to convert the categorical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_workclass = LabelEncoder()\n",
    "le_education = LabelEncoder()\n",
    "le_marital_status = LabelEncoder()\n",
    "le_occupation = LabelEncoder()\n",
    "le_relationship = LabelEncoder()\n",
    "le_race = LabelEncoder()\n",
    "le_sex = LabelEncoder()\n",
    "le_native_country = LabelEncoder()\n",
    "le_income = LabelEncoder()\n",
    "\n",
    "### To convert into numbers\n",
    "income_data.workclass = le_workclass.fit_transform(income_data.workclass)\n",
    "income_data.education = le_education.fit_transform(income_data.education)\n",
    "income_data.marital_status = le_marital_status.fit_transform(income_data.marital_status)\n",
    "income_data.occupation = le_occupation.fit_transform(income_data.occupation)\n",
    "income_data.relationship = le_relationship.fit_transform(income_data.relationship)\n",
    "income_data.race = le_race.fit_transform(income_data.race)\n",
    "income_data.sex = le_sex.fit_transform(income_data.sex)\n",
    "income_data.native_country = le_native_country.fit_transform(income_data.native_country)\n",
    "income_data.income = le_income.fit_transform(income_data.income)\n",
    "\n",
    "### To convert back for easy explaination\n",
    "# income_data.sex = le_sex.inverse_transform(income_data.sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [age, workclass, fnlwgt, education, education_num, marital_status, occupation, relationship, race, sex, capital_gain, capital_loss, hours_per_week, native_country, income]\n",
       "Index: []"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income_data[(income_data.capital_gain > 0) & (income_data.capital_loss > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the query above, we see there's no records with \"capital_gain\" and \"capital_loss\" greater than zero at the same time. It verified my assumption that the total amount of capital is shown in two seperated features -- when the number is positive it's shown in \"capital_gain\" and if it's negative then the absolute number is shown in \"capital_loss\". Therefore, I created a variable called \"capital_tot\" as following to summarise the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "income_data[\"capital_tot\"] = income_data.capital_gain - income_data.capital_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I want to narrow the candidates of features to choose from. I'd keep only one feature, the more informative one, from each highly correlated group of features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_sml = ['age', 'workclass', 'fnlwgt', \\\n",
    " 'education', 'marital_status', 'occupation',\\\n",
    " 'relationship', 'race', 'sex', 'capital_tot', \\\n",
    " 'hours_per_week', 'native_country']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The candidates are all the \"col_sml\" list shown above. \"education_num\" was removed since it's highly correlated with \"education\" and \"capital_gain\" and \"capital_loss\" were removed and only the newly created feature of \"capital_tot\" was kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_to_choose = income_data[col_sml].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def select_k_best_features(col_names, features, labels, k):\n",
    "    \"\"\"\n",
    "    For Census Income dataset, select k best features based on SelectKBest from \n",
    "    sklearn.feature_selection\n",
    "\n",
    "    Input:\n",
    "    dataset: data in dictionary format \n",
    "    k: the number of features to keep\n",
    "\n",
    "    Return:\n",
    "    the list of length of k: k best features \n",
    "    \"\"\"    \n",
    "    k_best = SelectKBest(k=k)\n",
    "    k_best.fit(features, labels)\n",
    "    impt_unsorted = zip(col_names, k_best.scores_)\n",
    "    impt_sorted = list(sorted(impt_unsorted, key=lambda x: x[1], reverse=True))\n",
    "    k_best_features = [elem[0] for elem in impt_sorted][:k]\n",
    "    print k, \"best features:\"\n",
    "    print k_best_features\n",
    "    return k_best_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use `SelectKBest` to get the k best features in term of their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 best features:\n",
      "['relationship', 'age', 'hours_per_week', 'sex', 'capital_tot', 'marital_status', 'education', 'occupation', 'race']\n"
     ]
    }
   ],
   "source": [
    "labels, features = income_data['income'].values, income_data[col_sml].values\n",
    "K_features = income_data[select_k_best_features(col_sml, features, labels, 9)].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the features and their scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 1885.3164412192941),\n",
       " ('workclass', 87.076858983064071),\n",
       " ('fnlwgt', 2.9380014706440551),\n",
       " ('education', 206.23794343878359),\n",
       " ('marital_status', 1344.3447614104623),\n",
       " ('occupation', 186.11728830088055),\n",
       " ('relationship', 2186.594101437629),\n",
       " ('race', 168.81906294077584),\n",
       " ('sex', 1591.7637612837962),\n",
       " ('capital_tot', 1568.1679650910089),\n",
       " ('hours_per_week', 1811.5338655278399),\n",
       " ('native_country', 7.9381403319498256)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_best = SelectKBest(k=10)\n",
    "k_best.fit(features, labels)\n",
    "impt_unsorted = zip(col_sml, k_best.scores_)\n",
    "impt_unsorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to keep nine features for future model(s) and discarded the three features ('workclass', 'fnlwgt', 'native_country') with score less than 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PERF_FORMAT_STRING = \"\\\n",
    "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "def test_classifier(clf, labels, features, folds = 100):\n",
    "    \"\"\"\n",
    "    Use cross validation to evaluate the performance of models\n",
    "    \"\"\"\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        \n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            else:\n",
    "                true_positives += 1\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print clf\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_parameter_from_search(pipeline, parameters, score_func, labels, features, kf = 10):\n",
    "    \"\"\" \n",
    "    print out the optimal parameters of pipeline classifier from grid search based on \n",
    "    score function of choice\n",
    "    \n",
    "    Input:\n",
    "    pipeline: classifier in pipeline form\n",
    "    parameters: the parameters to be grid searched\n",
    "    score_func: Scorer function used on the held out data to choose the best parameters for the model\n",
    "    dataset: data in dictionary format\n",
    "    features_list: the list of feature after feature selection\n",
    "    kf: kf-fold of cross validation for estimation\n",
    "    \"\"\"\n",
    "    ### Stratified ShuffleSplit cross validation iterator of the training set\n",
    "    cv_sss = StratifiedShuffleSplit(labels, n_iter=kf, test_size=0.2, random_state=0)\n",
    "\n",
    "    clf = GridSearchCV(pipeline, parameters, scoring=score_func, cv=cv_sss, n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    clf.fit(features, labels)\n",
    "    print \"done in %0.3fs\" % (time() - t0)\n",
    "    print\n",
    "    print(\"Best score: %0.3f\" % clf.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline_rfc = Pipeline(steps=[\n",
    "    ('classifier', RandomForestClassifier(random_state = 42))  \n",
    "])\n",
    "\n",
    "parameters_rfc = {\n",
    "    'classifier__max_features': ('sqrt', 1),\n",
    "    'classifier__max_depth': np.arange(3, 8),\n",
    "    'classifier__n_estimators' : (10, 20)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "('pipeline:', ['classifier'])\n",
      "parameters:\n",
      "{'classifier__max_depth': array([3, 4, 5, 6, 7]),\n",
      " 'classifier__max_features': ('sqrt', 1),\n",
      " 'classifier__n_estimators': (10, 20)}\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done 194 out of 200 | elapsed:   15.5s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   16.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 16.419s\n",
      "\n",
      "Best score: 0.628\n",
      "Best parameters set:\n",
      "\tclassifier__max_depth: 7\n",
      "\tclassifier__max_features: 'sqrt'\n",
      "\tclassifier__n_estimators: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(labels=[0 0 ..., 0 1], n_iter=10, test_size=0.2, random_state=0),\n",
       "       estimator=Pipeline(steps=[('classifier', RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
       "            criterion='gini', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0))]),\n",
       "       fit_params={}, iid=True, loss_func=None, n_jobs=-1,\n",
       "       param_grid={'classifier__max_features': ('sqrt', 1), 'classifier__n_estimators': (10, 20), 'classifier__max_depth': array([3, 4, 5, 6, 7])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring='f1',\n",
       "       verbose=1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameter_from_search(pipeline_rfc, parameters_rfc, 'f1', labels, K_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
      "            criterion='gini', max_depth=7, max_features='sqrt',\n",
      "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, n_estimators=20, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0)\n",
      "\tAccuracy: 0.85140\tPrecision: 0.79364\tRecall: 0.51788\tF1: 0.62677\tF2: 0.55656\n",
      "\tTotal predictions: 325400\tTrue positives: 40602\tFalse positives: 10557\tFalse negatives: 37798\tTrue negatives: 236443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_rfc = RandomForestClassifier(max_depth = 7, \n",
    "                             max_features = 'sqrt', \n",
    "                             n_estimators = 20, \n",
    "                             random_state = 42)\n",
    "test_classifier(clf_rfc, labels, K_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "('pipeline:', ['scaler', 'classifier'])\n",
      "parameters:\n",
      "{'classifier__C': array([  1.00000000e-10,   1.00000000e-09,   1.00000000e-08,\n",
      "         1.00000000e-07,   1.00000000e-06,   1.00000000e-05,\n",
      "         1.00000000e-04]),\n",
      " 'classifier__penalty': ('l1', 'l2')}\n",
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 134 out of 140 | elapsed:    4.8s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    4.8s finished\n",
      "/Users/apple/anaconda/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/apple/anaconda/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/apple/anaconda/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/apple/anaconda/lib/python2.7/site-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 4.986s\n",
      "\n",
      "Best score: 0.520\n",
      "Best parameters set:\n",
      "\tclassifier__C: 9.9999999999999995e-08\n",
      "\tclassifier__penalty: 'l2'\n"
     ]
    }
   ],
   "source": [
    "pipeline_lrg = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(tol = 0.001, random_state = 42))\n",
    "])\n",
    "\n",
    "parameters_lrg = {\n",
    "    'classifier__penalty': ('l1', 'l2'),\n",
    "    'classifier__C': 10.0 ** np.arange(-10, -3)\n",
    "    }\n",
    "\n",
    "### Grid search for the optimal parameters\n",
    "best_parameter_from_search(pipeline_lrg, parameters_lrg, 'f1', labels, K_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('classifier', LogisticRegression(C=1e-08, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, penalty='l2', random_state=42, tol=0.001))])\n",
      "\tAccuracy: 0.77170\tPrecision: 0.52665\tRecall: 0.51806\tF1: 0.52232\tF2: 0.51976\n",
      "\tTotal predictions: 325400\tTrue positives: 40616\tFalse positives: 36505\tFalse negatives: 37784\tTrue negatives: 210495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_lrg = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(tol = 0.001, C = 10**-8, penalty = 'l2', \n",
    "                                          random_state = 42))\n",
    "])\n",
    "test_classifier(clf_lrg, labels, K_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Read id data\n",
    "income_data_test = pd.read_csv(\"adult.test.csv\", names = colnames,\n",
    "                              skiprows = 1)\n",
    "\n",
    "### Convert the categorical variables\n",
    "income_data_test.workclass = le_workclass.fit_transform(income_data_test.workclass)\n",
    "income_data_test.education = le_education.fit_transform(income_data_test.education)\n",
    "income_data_test.marital_status = le_marital_status.fit_transform(income_data_test.marital_status)\n",
    "income_data_test.occupation = le_occupation.fit_transform(income_data_test.occupation)\n",
    "income_data_test.relationship = le_relationship.fit_transform(income_data_test.relationship)\n",
    "income_data_test.race = le_race.fit_transform(income_data_test.race)\n",
    "income_data_test.sex = le_sex.fit_transform(income_data_test.sex)\n",
    "income_data_test.native_country = le_native_country.fit_transform(income_data_test.native_country)\n",
    "income_data_test.income = le_income.fit_transform(income_data_test.income)\n",
    "\n",
    "### Add new feature\n",
    "income_data_test[\"capital_tot\"] = income_data_test.capital_gain - income_data_test.capital_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 best features:\n",
      "['relationship', 'hours_per_week', 'age', 'capital_tot', 'sex', 'marital_status', 'education', 'occupation', 'race']\n"
     ]
    }
   ],
   "source": [
    "labels_test, features_test = income_data_test['income'].values, income_data_test[col_sml].values\n",
    "K_features_test = income_data_test[select_k_best_features(col_sml, features_test, labels_test, 9)].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
      "            criterion='gini', max_depth=7, max_features='sqrt',\n",
      "            max_leaf_nodes=None, min_density=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, n_estimators=20, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0)\n",
      "\tAccuracy: 0.84900\tPrecision: 0.81129\tRecall: 0.47055\tF1: 0.59563\tF2: 0.51370\n",
      "\tTotal predictions: 162900\tTrue positives: 18116\tFalse positives: 4214\tFalse negatives: 20384\tTrue negatives: 120186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(clf_rfc, labels_test, K_features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction on Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('classifier', LogisticRegression(C=1e-08, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, penalty='l2', random_state=42, tol=0.001))])\n",
      "\tAccuracy: 0.77023\tPrecision: 0.51451\tRecall: 0.49283\tF1: 0.50344\tF2: 0.49702\n",
      "\tTotal predictions: 162900\tTrue positives: 18974\tFalse positives: 17904\tFalse negatives: 19526\tTrue negatives: 106496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(clf_lrg, labels_test, K_features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Conclusion and reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, Random Forest Classifier overperforms Logistic Regression on prediction of income type. Also, we see the \"accuracy\", \"recall\", \"precision\" scores of each model were really consistent for both the training set and the test set, which means the training dataset is really representitave and our model were neither overfitting nor underfitting.  \n",
    "\n",
    "What to improve next: \n",
    "1. Due to the time limit, I didn't investigate the outliers at the beginning. However, this step is really important in terms of parameter tuning.\n",
    "2. Also, doing the correlation analysis first may also be a good idea since many algorithms, epecially Naive Bayes depends on the assumption that all the features are independent. Although I removed some features before carrying out `SelectKBest` intuitively to avoid the problem may caused by multicolinearity.\n",
    "3. It would be interesting to try other algorithms as well, such as Naive Bayes and Support Vector Classifier etc. and compare the preformance of different models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
